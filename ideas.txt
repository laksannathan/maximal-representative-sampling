
The specific case where the probability of attribute values is different while the conditional distribution of the class variable given the attributes remains unchanged is a non-stationary environment known as covariate shift.

trying to estimate a function f which is positive on S and negative on the complement.

The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.
(Estimating the Support of a High-Dimensional Distribution 
November 1, 1999 ) John Platt

The dataset that you use for training can contain all or mostly normal cases.

Typically, the SVM algorithm is given a set of training examples labeled as belonging to one of two classes.
 An SVM model is based on dividing the training sample points into separate categories by as wide a gap as possible
 , while penalizing training samples that fall on the wrong side of the gap.
 The SVM model then makes predictions by assigning points to one side of the gap or the other.

 Therefore, in one-class SVM, the support vector model is trained on data that has only one class, which is the “normal” class.
  It infers the properties of normal cases and from these properties can predict which examples are unlike the normal examples. 

Type a value that represents the upper bound on the fraction of outliers.
 This parameter corresponds to the nu-property described in this paper. 
 The nu-property lets you control the trade-off between outliers and normal cases.

e (epsilon): Type a value to use as the stopping tolerance. The stopping tolerance,
 affects the number of iterations used when optimizing the model, and depends on the stopping criterion value. 
When the value is exceeded, the trainer stops iterating on a solution.


Predictions from the One-Class SVM are uncalibrated scores that may be possibly unbounded
 be sure to normalize scores if you are comparing models based on different algorithms.

 and the approach towards one-class support vector machines, is described in these papers by B. Sch?lkopf et al.

 nu: This parameter (represented by the Greek letter nu) determines the trade-off between the fraction of outliers and the number of support vectors.

 epsilon: Specifies the stopping tolerance.

while the threshold can be controlled by the contamination parameter

----------------

Particularly, statistical bounds like Hoeffding and Vapnik-Chervonenkis require a match between training and testing distributions.
Chi Square goodness of fit test and Kolmogorov.

for relating training samples to test data.
The poll indicated that, […]
The classification process regarding political participation

inadvertently

The predictive variables and their values must be mapped to appropriate scales between GESIS and GBS.

The classifier ends up using the minimum-error attribute for prediction.

John Platt's sequential minimal optimization algorithm 
for training a support vector machine.

the only case where it is possible to have instances classified as GBS
with a ratio of under 1:5 is when the algorithm excluded a subgroup
of the population that was overrepresented in GBS.

However, if we exclude all instances, then we are also changing the GESIS
distribution, of which we know it is representative.
subsampling should go one way only: adapting GBS distribution.

"exclude only the really sure ones"

Example in-text citation:
Inner city mental health care access continues to be a problem (see Appendix for a table showing mental health care access by city).

Aus <http://askus.library.wwu.edu/faq/116707> 

http://ccsg.isr.umich.edu/index.php/chapters/statistical-analysis-chapter#nine


But if the dev and test sets come from different distributions, then your options are less clear. Several things could have gone wrong:  
1. You had overfit to the dev set.  2. The test set is harder than the dev set. So your algorithm might be doing as well as could be expected, and no further significant improvement is possible.
3. The test set is not necessarily harder, but just different, from the dev set. So what works well on the dev set just does not work well on the test set. In this case, a lot of your work to improve dev set performance might be wasted effort.  
Working on machine learning applications is hard enough. Having mismatched dev and test sets introduces additional uncertainty about whether improving on the dev set distribution also improves test set performance. Having mismatched dev and test sets makes it harder to figure out what is and isn’t working, and thus makes it harder to prioritize what to work on.  
If you are working on a 3rd party benchmark problem, their creator might have specified dev and test sets that come from different distributions. Luck, rather than skill, will have a greater impact on your performance on such benchmarks compared to if the dev and test sets come from the same distribution. It is an important research problem to develop learning algorithms that are trained on one distribution and generalize well to another. But if your goal is to make progress on a specific machine learning application rather than make research progress, I  recommend trying to choose dev and test sets that are drawn from the same distribution. This will make your team more efficient. 


Having multiple-number evaluation metrics makes it harder to compare algorithms. Suppose your algorithms perform as follows:  
 
Classifier  Precision  Recall  A  95% 90% B  98% 85% 
 
Here, neither classifier is obviously superior, so it doesn’t immediately guide you toward picking one. 
 
Classifier  Precision  Recall  F1 score  A  95% 90% 92.4% 
 
During development, your team will try a lot of ideas about algorithm architecture, model parameters, choice of features, etc. Having a ?single-number evaluation metric? such as accuracy allows you to sort all your models according to their performance on this metric, and quickly decide what is working best.  
If you really care about both Precision and Recall, I recommend using one of the standard ways to combine them into a single number. For example, one could take the average of precision and recall, to end up with a single number. 


Having a single-number evaluation metric speeds up your ability to make a decision when you are selecting among a large number of classifiers. It gives a clear preference ranking among all of them, and therefore a clear direction for progress

(BUT SINCE ACCURACY IS NOT A GOOD INDICATOR FOR PERFORMANCE IN HIGHLY IMBALANCED SETTINGS. WE REFER TO AUROC.)  



Ryan Tibshirani reminded me that there are plenty of Machine Learning approaches that are not i.i.d.! For instance, there’s sequential or online learning (your data are coming in a sequence that is not i.i.d., may be changing over time, and might even be given to you by an “adversary” who tries to keep it as uninformative as possible) and active learning (you get to see some data, fit a preliminary model, and then choose where to see the next data point: how best to choose that next observation if you want to maximize your chance of learning something useful?) And of course time series or spatial data are another non-i.i.d. setup frequently used in ML

