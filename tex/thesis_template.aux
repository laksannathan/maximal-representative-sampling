\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plain}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\thispagestyle {empty}}
\citation{tuscher}
\citation{candela}
\citation{west}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{denis,claesen}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{project}{{\caption@xref {project}{ on input line 25}}{2}{Introduction}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Multivariate auxiliary information GESIS linked to GBS so that expected bias can be detected and corrected for. In addition, GBS contains an attribute for positive or negative treatment of survey participants for further analysis.\relax }}{2}{figure.caption.4}}
\citation{jean,shimodaira,brick}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Related Work}{3}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Outline}{3}{section.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Initial Data Analysis}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Sec:Initial Data Analysis}{{2}{5}{Initial Data Analysis}{chapter.2}{}}
\newlabel{std}{{\caption@xref {std}{ on input line 9}}{5}{Initial Data Analysis}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces GBS - GESIS attribute and value comparison. Not all attributes are used in every learning task. See GitHub documentation for more information.\relax }}{5}{figure.caption.5}}
\citation{rammstedt}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Feature Selection and Data Imputation}{6}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Perspectives on Response Styles}{6}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Likert-Type Scale}{6}{subsection.2.2.1}}
\citation{likert}
\citation{likert2,likert3}
\citation{likert4}
\citation{likert4}
\citation{heeringa}
\newlabel{Conscientiousness}{{\caption@xref {Conscientiousness}{ on input line 61}}{7}{Likert-Type Scale}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Conscientiousness is the degree of organization, self-regulation, and responsibility one exhibits. \textit  {"I see myself as someone who tends to be lazy."}(left). \textit  {"I see myself as someone who does a thorough job."}(right). The graphs are almost identical for the Likert item "Gruendlich". Respondents specify their level of disagreement for "Faulheit" stronger in GBS.\relax }}{7}{figure.caption.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Data Mismatch}{7}{subsection.2.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces "Wach" as an example of a Likert item discrepancy. GESIS uses an odd number of responses with a "neutral" option, such as "no opinion", "neither agree nor disagree" or some phrase to that effect. In contrast, there is an even number of responses for this item in GBS encouraging participants to voice a positive or negative opinion. In some cases, an additional "opt-out" option is provided for those respondents who truly cannot respond. The respondent may not respond because some questions are too sensitive. This type of nonresponse would also be considered missing values indicated by "-1" \cite  {likert4}.\relax }}{8}{table.caption.10}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Demonstration of potential scalings for differing attribute values. The cut-off mapping comes with a loss of information. Max scaler and min-max scaler introduce new unseen values that can be problematic for statistical learning.\relax }}{8}{table.caption.11}}
\newlabel{Tab:DescripStatsRawData}{{2.2}{8}{Demonstration of potential scalings for differing attribute values. The cut-off mapping comes with a loss of information. Max scaler and min-max scaler introduce new unseen values that can be problematic for statistical learning.\relax }{table.caption.11}{}}
\newlabel{gesis_miss}{{\caption@xref {gesis_miss}{ on input line 24}}{9}{Feature Selection and Data Imputation}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Missing values in GESIS. The attribute values of a participant are always known for "Geschlecht", "Geburtsland", "Geburtsjahr", "Nationalitaet", "Familienstand", "Personen im Haushalt". In contrast, the last three columns "Druck", "Optimismus Zukunft", "Zufriedenheit Wahlergebnisse" and "Resilienz" are almost always missing and are therefore removed from the analysis. Participants with missing BFI-10 elements are removed. Sample size will only be reduced slightly as missing values often occur for the same instance. These dependencies form a line pattern in the graph. "Berufsgruppe" was surveyed as a text field so that the column clearly suffers from ambiguous value mismatch. To include "Berufsgruppe" mappings need to be redefined first. For now, "Berufsgruppe" is removed. \relax }}{9}{figure.caption.6}}
\newlabel{gbs_miss}{{\caption@xref {gbs_miss}{ on input line 33}}{10}{Feature Selection and Data Imputation}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Missing values in GBS. There is one more attribute in "Gruppe". Not every participant received a positive a negative psychological treatment. Therefore, "Gruppe" is more likely to be missing than not. However, the absence of a value indicates no treatment rather than a missing positive or negative. "Gruppe" is not properly represented yet. "Desinteresse Politiker" is given by multiple data sources from different excel files. Some of them being the inverse of the attribute itself. The surey design regarding this issue is unclear to me. To incorporate "Desinteresse Politiker" the attribute(s) need to be imported correctly, if possible. Another import issue is given by "Personen im Haushalt". If the actual value is greater than one, the cell will be empty. To correct this, the corresponding csv-file needs to be fixed. The text field "Berufsgruppe" suffers on both ends, GBS and GESIS, due to current oversimplification of value and potential data mismatch.\relax }}{10}{figure.caption.7}}
\newlabel{corr}{{\caption@xref {corr}{ on input line 42}}{11}{Feature Selection and Data Imputation}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The upper right triangular matrix shows GESIS correlations while GBS correlations are shown in the lower left. The main diagonal should not be confused with white squares. These trivial combinations are simply excluded and not colored black. As can be seen "Personen im Haushalt" in GBS can not be calculated, since there is only one possible value. "Nettoeinkommen Selbst" and "Nettoeinkommen Haushalt" are highly correlated but not removed or handled at all. I will keep this in mind, when facing the naive bayes assumption in the learning process. Entropy-based mutual information in "Wahlteilnahme" and "Wahlabsicht" have led to almost perfect classification performances in predicting political participation. "Wahlabsicht" is therefore removed.\relax }}{11}{figure.caption.8}}
\newlabel{std}{{\caption@xref {std}{ on input line 119}}{12}{Data Mismatch}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Overview of attributes that can not be used for further analysis due to data mismatch. GESIS histograms (right) are show differently scaled values, some of which have been transformed already. Taking the inverse of "Aengstlich", "Alles anstrengend" and perhaps "Veraergert" for either the left or right side is likely to match the values properly. The remaining attributes do not seem to be reliable and will not be included for the time being. Depending on the discriminative algorithm, values ​​that only appear in one of the two surveys are enough to perfectly classify instances: "\textit  {If value equals 1.7 \(\rightarrow \) Instance of class GESIS}". Decision-tree based learning algorithms will likely suffer from erroneous mappings, while logistic regression with a proper scoring rule might be the most suitable algorithm to limit the effects of measurement discrepancy.\relax }}{12}{figure.caption.12}}
\citation{tom}
\citation{west}
\citation{yaser}
\citation{yaser}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Feasibility of Learning}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Sec:Feasibility of Learning}{{3}{13}{Feasibility of Learning}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Sampling Bias}{13}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The Problem of Overfitting}{13}{section.3.2}}
\citation{yaser}
\citation{ian}
\citation{yaser}
\citation{elkan,claesen}
\citation{jain,claesen}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Learning from Positive and Unlabeled Data}{15}{section.3.3}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces PU training procedure\relax }}{15}{algocf.1}}
\newlabel{alg:alg}{{1}{15}{Learning from Positive and Unlabeled Data}{algocf.1}{}}
\citation{roc}
\newlabel{project}{{\caption@xref {project}{ on input line 43}}{16}{Learning from Positive and Unlabeled Data}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Tree-based algorithm as discriminative learner. Increasing model complexity results in a better separation of GBS and GESIS and an improvement of the AUROC on the training data. Predicted probabilities are based on the proportions of positives and negatives in subspaces. Test set instances of GBS in leaf nodes are removed if correctly classified. The terms over-represented and under-represented can be translated to four distinct leaf node possibilities. If we excluded all misclassified instances, then we are also changing GESIS distribution, of which we know it is representative. Subsampling means adapting GBS distributions only.\relax }}{16}{figure.caption.13}}
\citation{leo}
\citation{claesen2,jain2}
\citation{claesen2,jain,jain2}
\citation{jain}
\citation{jain}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{19}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Sec:Results}{{4}{19}{Results}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Processing Data with Decision-Trees}{19}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Trees were trained using either sci-kit learn or WEKA explorer to identify preprocessing mistakes. The outer right path of the decision-tree demonstrates the desired behavior of the constructed model. "Resilienz" and "Zufriedenheit Wahlergebnis" have been used to classify the majority of instances as GESIS. Both attributes have been measured on the same scale or mapped properly. Technically, this means that there was no better way to split the data at that specific node. Either the algorithm did not see any gain in continuing to split or the tree was fully grown first and then pruned back using an out-of sample estimations.\relax }}{19}{figure.caption.14}}
\citation{claesen2}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Some descriptive statistics of location and dispersion for 2100 observed swap rates for the period from February 15, 1999 to March 2, 2007. Swap rates measured as 3.12 (instead of 0.0312). See Table \ref  {Tab:DescripStatsRawDataDetail} in the appendix for more details.\relax }}{20}{table.caption.15}}
\newlabel{Tab:DescripStatsRawData}{{4.1}{20}{Some descriptive statistics of location and dispersion for 2100 observed swap rates for the period from February 15, 1999 to March 2, 2007. Swap rates measured as 3.12 (instead of 0.0312). See Table \ref {Tab:DescripStatsRawDataDetail} in the appendix for more details.\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Estimating the Fraction of Representatives}{20}{section.4.2}}
\citation{breiman2}
\citation{breiman2}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Tuning parameter nu that controls the trade-oﬀ between the fraction of non-representative samples and the number of support vectors in one-class SVM. More than 0.73 of GBS (right) are classiﬁed as representative with low conﬁdence (high sdt.) for the optimal value \( \nu = 10^5\).\relax }}{21}{figure.caption.16}}
\newlabel{fig:Ng1}{{4.2}{21}{Tuning parameter nu that controls the trade-oﬀ between the fraction of non-representative samples and the number of support vectors in one-class SVM. More than 0.73 of GBS (right) are classiﬁed as representative with low conﬁdence (high sdt.) for the optimal value \( \nu = 10^5\).\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Maximal Representative Subsample}{21}{section.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces GridSearchCV: Gridsearch using repeated 10-fold stratified cross-validation on \(\frac  {1}{2}\) of the data to evaluate multiple scorers simultaneously. The baselines are 0.89 for accuracy, 0.82 for AUC/AUROC and 0.29 for logarithmic loss. The baselines outperform the trained models on the given metrics, due to the high imbalance in the data.\relax }}{22}{figure.caption.17}}
\newlabel{fig:Ng1}{{4.3}{22}{GridSearchCV: Gridsearch using repeated 10-fold stratified cross-validation on \(\frac {1}{2}\) of the data to evaluate multiple scorers simultaneously. The baselines are 0.89 for accuracy, 0.82 for AUC/AUROC and 0.29 for logarithmic loss. The baselines outperform the trained models on the given metrics, due to the high imbalance in the data.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces ROC and puROC evaluation for PU learning with Random Forests as base models \cite  {breiman2}. An AUROC of approximately \(\frac  {1}{2}\) implies that there is no more evidence for covariate shift.\relax }}{22}{figure.caption.18}}
\newlabel{fig:Ng1}{{4.4}{22}{ROC and puROC evaluation for PU learning with Random Forests as base models \cite {breiman2}. An AUROC of approximately \(\frac {1}{2}\) implies that there is no more evidence for covariate shift.\relax }{figure.caption.18}{}}
\newlabel{fig:Ng1}{{\caption@xref {fig:Ng1}{ on input line 80}}{23}{Maximal Representative Subsample}{figure.caption.19}{}}
\newlabel{sub@fig:Ng1}{{}{23}{Maximal Representative Subsample}{figure.caption.19}{}}
\newlabel{fig:Ng2}{{\caption@xref {fig:Ng2}{ on input line 85}}{23}{Maximal Representative Subsample}{figure.caption.19}{}}
\newlabel{sub@fig:Ng2}{{}{23}{Maximal Representative Subsample}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Feature importance in GBS (n=579) and GBS MRS (n=280) for classification of political participation "Wahlteilnahme". In modelling a political participation process, algorithms approximate the likelihood of a person going to vote on election day. Ideally, for every instance with unknown political interest and willingness to participate, there is enough data of people of similiar demographics, socioeconomics and psychological traits to generalize from.\relax }}{23}{figure.caption.19}}
\citation{smote}
\citation{jesse}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Future Work}{25}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{std}{{\caption@xref {std}{ on input line 13}}{25}{Future Work}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Artificial data synthesis to overrepresent subgroups of GESIS. True negatives are removed from the MRS with positive classes GESIS (left) and GESIS-1 (right). Oversampled instances can easily be marked as such for result set comparisons.\relax }}{25}{figure.caption.20}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{27}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{Sec:Conclusion}{{6}{27}{Conclusion}{chapter.6}{}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{27}{chapter.6}}
\bibcite{tuscher}{1}
\bibcite{rammstedt}{2}
\bibcite{candela}{3}
\bibcite{west}{4}
\bibcite{heeringa}{5}
\bibcite{likert}{6}
\bibcite{likert3}{7}
\bibcite{likert4}{8}
\bibcite{roc}{9}
\bibcite{jean}{10}
\bibcite{shimodaira}{11}
\bibcite{denis}{12}
\bibcite{claesen}{13}
\bibcite{claesen2}{14}
\bibcite{jain}{15}
\bibcite{jain2}{16}
\bibcite{bickel}{17}
\bibcite{tom}{18}
\bibcite{Sechidis}{19}
\bibcite{elkan}{20}
\bibcite{brick}{21}
\bibcite{shimodaira}{22}
\bibcite{leo}{23}
\bibcite{trevor}{24}
\bibcite{ian}{25}
\bibcite{yaser}{26}
\bibcite{jesse}{27}
\bibcite{walker}{28}
\bibcite{cox}{29}
\bibcite{breiman2}{30}
\bibcite{smote}{31}
