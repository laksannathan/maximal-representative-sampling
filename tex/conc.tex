\chapter{Conclusion}\label{Sec:Conclusion}

Although model learning and performance evaluation in a supervised setting are well understood, the availability of unlabeled data gives additional optionsand also presents new challenges.
Imbalanced setting where the minority class GBS is the unlabeled class of interest but handled as negative. evaluation in the absence of actual negatives is further enhanced by class imbalance. puF-Measure instead of (or in addition to) puROC could reduce the effects of class imbalance. inclusion of f1-measure would be interesting. finally, the label "representative" is actually a property of a sample and not its single instances. survey mismatch from gbs to gesis renders most attributes useless regarding entropy. forces valu comparisons essentially introduce non existent pattern. one class classification suffers from high variance in estimating the fraction of actually representative gbs data. assessment of binary classifiers in pu settings does not lead to more accurate roc curve estimations as the fraction of positives cannot be estimated. overrepresentative underrep. rep and nonrep. trhoughout the thesis are not properly defined.

This paper presents a theoretical analysis of sample selection bias correction. The sample bias correction technique commonly used in machine learning consists of reweighting the cost of an error on each training point of a biased sample to more closely reflect the unbiased distribution. This relies on weights derived by various estimation techniques based on finite samples. We analyze the effect of an error in that estimation on the accuracy of the hypothesis returned by the learning algorithm for two estimation techniques: a cluster-based estimation technique and kernel mean matching. We also report the results of sample bias correction experiments with several data sets using these techniques. Our analysis is based on the novel concept of <em>distributional stability</em>which generalizes the existing concept of point-based stability. Much of our work and proof techniques can be used to analyze other importance weighting techniques and their effect on accuracy when using a distributionally stable algorithm. C. Cortes, M. Mohri, M. Riley, and A. Rostamizadeh. Sample selection bias correction theory. In Proceedings of the International Conference on Algorithmic Learning Theory, 2008.